For the last assignment of the Deep Learning module, we decided to try and finetune a GPT2 model on transcripts of the Huberman Lab Podcast. For this purpose we scraped 118 PDF's from readthatpodcast.com, preprocessed the text into training and validation .txt files and then ran the model. We experienced some difficulty with Google Collab timing out and GPU's crashing, but were eventually able to get a full training loop completed with 15 epochs. The finished model was still not the most coherent, but definitely did perform better than our initial progress.

We would expect the model to perform better if we could manage to train it for more epochs, and with adjusting some of the other hyperparameters to better adjust the weights of the model. Some of the preprocessing steps might have influenced the outputs in ways we didn't necessarily expect as well, as the outputs can contain unnecessary spaces or symbols, which could potentially also be affecting the coherence of the model.
