For the last assignment of the Deep Learning module, we decided to try and finetune a GPT2 model on transcripts of the Huberman Lab Podcast. For this purpose we scraped 118 PDF's from readthatpodcast.com, preprocessed the text into training and validation .txt files and then ran the model. We experienced some difficulty with Google Collab timing out and GPU's crashing, but were eventually able to get a full training loop completed with 20 epochs. The finished model was still not the most coherent, but definitely did perform better than our initial progress.

We would expect the model to perform better if we could manage to train it for more epochs, and with adjusting some of the other hyperparameters to better adjust the weights of the model. Some of the preprocessing steps might have influenced the outputs in ways we didn't necessarily expect as well, as the outputs can contain unnecessary spaces or symbols, which could potentially also be affecting the coherence of the model. Looking at the outputs, we can see that we didn't manage to remove all filler words such as "This podcast was transcribed by readthatpodcast.com" etc. So this task would definitely be first on the list before further training.

We played around with different batch_sizes, learning rates and other parameters in order to reduce the training loss in order to get the model to converge.
On our last run we reach a training loss of 2.629400, but with the model still learning at a decently fast rate we doubt that this is the "plateau" at which the model converges.
Sadly we weren't able to achieve this, as we simply couldn't get Google Collab to play ball, and setting the parameters to something that would cause a 3+ hours training loop would more often than not cause the notebook to time out and lose our progress. 
